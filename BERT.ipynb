{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import collections\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import BertTokenizerFast,BertModel,BertPreTrainedModel,AdamW\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(train=True):\n",
    "    num_article = len(data['data'])\n",
    "    if train:\n",
    "        temp_data = data['data'][:(9*num_article//10)]\n",
    "    else:\n",
    "        temp_data = data['data'][(9*num_article//10):]\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    for article in temp_data:\n",
    "        for p in article['paragraphs']:\n",
    "            context = p['context']\n",
    "            for qa in p['qas']:\n",
    "                question = qa['question']\n",
    "                contexts.append(context)\n",
    "                questions.append(question)\n",
    "                if qa['is_impossible']:\n",
    "                    answers.append({'answer_start': 0, 'text': ''})\n",
    "                else:\n",
    "                    answers.append(qa['answers'][0])\n",
    "    return contexts, questions, answers\n",
    "\n",
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     \n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2\n",
    "            \n",
    "def preprocess(contexts,questions,answers,train=True):\n",
    "    context_list = []\n",
    "    question_list = []\n",
    "    start_position = []\n",
    "    end_position = []\n",
    "    for i in tqdm(range(len(contexts))):\n",
    "        encoding = tokenizer(contexts[i])\n",
    "        token_list = tokenizer.convert_ids_to_tokens(encoding['input_ids'],skip_special_tokens=True)\n",
    "        context_list.append(['[UNK]']+token_list)\n",
    "        if train:\n",
    "            if answers[i]['text'] == '':\n",
    "                start_position.append(0)\n",
    "                end_position.append(0)\n",
    "            else:\n",
    "                start = encoding.char_to_token(answers[i]['answer_start'])\n",
    "                end = encoding.char_to_token(answers[i]['answer_end']-1)\n",
    "                start_position.append(start)\n",
    "                end_position.append(end)\n",
    "    for i in tqdm(range(len(questions))):\n",
    "        encoding = tokenizer(questions[i])\n",
    "        token_list = tokenizer.convert_ids_to_tokens(encoding['input_ids'],skip_special_tokens=True)\n",
    "        question_list.append(token_list)\n",
    "    if train:\n",
    "        return context_list,question_list,start_position,end_position\n",
    "    else:\n",
    "        return context_list,question_list\n",
    "    \n",
    "def remove_articles(text):\n",
    "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "    return re.sub(regex, ' ', text)\n",
    "\n",
    "def white_space_fix(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def normalize_answer(s):\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_exact(a_gold, a_pred):\n",
    "    if normalize_answer(a_gold) == normalize_answer(a_pred):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_tokens = get_tokens(a_gold)\n",
    "    pred_tokens = get_tokens(a_pred)\n",
    "    overlap = collections.Counter(gold_tokens) & collections.Counter(pred_tokens)\n",
    "    num_overlap = sum(overlap.values())\n",
    "    if len(gold_tokens) == 0 or len(pred_tokens) == 0:\n",
    "        if gold_tokens == pred_tokens:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    if num_overlap == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_overlap / len(pred_tokens)\n",
    "    recall = 1.0 * num_overlap / len(gold_tokens)\n",
    "    f1 = (2.0 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        if answers[i]['answer_start']==0 and answers[i]['answer_end']==0:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:     \n",
    "            start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "            end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "            # if None, the answer passage has been truncated\n",
    "            if start_positions[-1] is None:\n",
    "                start_positions[-1] = max_length-1\n",
    "            if end_positions[-1] is None:\n",
    "                end_positions[-1] = max_length-1\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    \n",
    "def evaluate(model, dataset):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        model.to(device)\n",
    "        em = 0.0\n",
    "        f1 = 0.0\n",
    "        for i in tqdm(range(len(dataset))):\n",
    "            temp_data = dataset[i]\n",
    "            input_ids = temp_data['input_ids'].to(device).unsqueeze(0)\n",
    "            attention_mask = temp_data['attention_mask'].to(device).unsqueeze(0)\n",
    "            token_type_ids = temp_data['token_type_ids'].to(device).unsqueeze(0)\n",
    "            start_score,end_score = model.get_scores(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            start_score = start_score.squeeze(0).cpu()\n",
    "            end_score = end_score.squeeze(0).cpu()\n",
    "            answer_start = torch.argmax(start_score).item()\n",
    "            answer_end = torch.argmax(end_score).item()\n",
    "            pred = ''\n",
    "            length = start_score.size(0)\n",
    "            if answer_start == 0 or answer_end == 0 or answer_start==(length-1) or answer_end==(length-1):\n",
    "                pred = ''\n",
    "            elif answer_end < answer_start:\n",
    "                pred = ''\n",
    "            elif answer_end - answer_start > 20:\n",
    "                pred = ''\n",
    "            else:\n",
    "                input_ids.cpu()\n",
    "                pred = tokenizer.decode(input_ids[0][answer_start:(answer_end+1)])\n",
    "            gold_text = dataset[i]['gold_text']\n",
    "            em += compute_exact(gold_text,pred)\n",
    "            f1 += compute_f1(gold_text,pred)\n",
    "        em /= len(dataset)\n",
    "        f1 /= len(dataset)\n",
    "        print('EM: %.5f'%em)\n",
    "        print('F1: %.5f'%f1)\n",
    "        return em, f1\n",
    "    \n",
    "def online_predict(question, context):\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    with torch.no_grad():\n",
    "        temp_encoding = tokenizer(context,question,truncation=True,padding=True)\n",
    "        input_ids = torch.LongTensor(temp_encoding['input_ids']).unsqueeze(0).to(device)\n",
    "        token_type_ids = torch.LongTensor(temp_encoding['token_type_ids']).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.LongTensor(temp_encoding['attention_mask']).unsqueeze(0).to(device)\n",
    "        start_score, end_score = model_bert.get_scores(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        start_score = start_score.squeeze(0).cpu()\n",
    "        end_score = end_score.squeeze(0).cpu()\n",
    "        answer_start = torch.argmax(start_score).item()\n",
    "        answer_end = torch.argmax(end_score).item()\n",
    "        pred = ''\n",
    "        length = start_score.size(0)\n",
    "        if answer_start == 0 or answer_end == 0 or answer_start==(length-1) or answer_end==(length-1):\n",
    "            pred = ''\n",
    "        elif answer_end < answer_start:\n",
    "            pred = ''\n",
    "        elif answer_end - answer_start > 20:\n",
    "            pred = ''\n",
    "        else:\n",
    "            input_ids.cpu()\n",
    "            pred = tokenizer.decode(input_ids[0][answer_start:(answer_end+1)])\n",
    "        return pred, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../input/squad-20/train-v2.0.json'\n",
    "f = open(train_path,'r')\n",
    "data = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts, train_questions, train_answers = read_data()\n",
    "add_end_idx(train_answers, train_contexts)\n",
    "train_context_list,train_question_list,train_start_position,train_end_position = preprocess(train_contexts,train_questions,train_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_contexts, val_questions, val_answers = read_data(train=False)\n",
    "val_context_list,val_question_list = preprocess(val_contexts,val_questions,val_answers,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count = len(train_answers)\n",
    "na_count = 0\n",
    "for i in range(total_count):\n",
    "    if train_answers[i]['text']=='':\n",
    "        na_count+=1\n",
    "print('total questions: %d'%(total_count))\n",
    "print('un-answered questions: %d'%na_count)\n",
    "print('un-answered percent: %.2f'%(100*(na_count/total_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding='max_length', max_length=max_length)\n",
    "add_token_positions(train_encodings, train_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(torch.utils.data.Dataset):   \n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset_Validation(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, answers):\n",
    "        self.encodings = encodings\n",
    "        self.answers = answers\n",
    "    def __getitem__(self, idx):\n",
    "        return_dict = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return_dict['gold_text'] = self.answers[idx]['text']\n",
    "        return return_dict\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding='max_length', max_length=max_length)\n",
    "val_dataset = SquadDataset_Validation(val_encodings, val_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert_QA(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.fc = nn.Linear(config.hidden_size, 2)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        token_type_ids,\n",
    "        start_positions,\n",
    "        end_positions,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state \n",
    "        logits = self.fc(sequence_output) \n",
    "        context_mask = (attention_mask-token_type_ids).unsqueeze(-1)\n",
    "        logits = logits + (context_mask + 1e-45).log()\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "        start_loss = self.criterion(start_logits, start_positions)\n",
    "        end_loss = self.criterion(end_logits, end_positions)\n",
    "        loss = start_loss + end_loss\n",
    "        return loss\n",
    "\n",
    "    def get_scores(self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        token_type_ids\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        logits = self.fc(sequence_output) \n",
    "        context_mask = (attention_mask-token_type_ids).unsqueeze(-1)\n",
    "        logits = logits + (context_mask + 1e-45).log()\n",
    "        start_logits, end_logits = logits.split(1, dim=-1) \n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1) \n",
    "        start_score = nn.Softmax(dim=1)(start_logits)\n",
    "        end_score = nn.Softmax(dim=1)(end_logits)\n",
    "        return start_score,end_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = Bert_QA.from_pretrained('bert-base-uncased')\n",
    "model.to(device)\n",
    "model.train()\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "train_loader = DataLoader(train_dataset, batch_size=24, shuffle=True, drop_last=True)\n",
    "optim = AdamW(model.parameters(), lr=3e-5)\n",
    "iter_counter = 0\n",
    "best_f1 = 0\n",
    "avg_loss = 0\n",
    "\n",
    "for epoch in range(3):\n",
    "    print('epoch %d start!'%(epoch+1))\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        loss = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions, token_type_ids=token_type_ids)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        iter_counter += 1\n",
    "        avg_loss += loss.item()\n",
    "        if iter_counter%100 == 0:\n",
    "            avg_loss /= 100\n",
    "            print('iter %d'%iter_counter)\n",
    "            print('loss %.5f'%avg_loss)\n",
    "            avg_loss = 0\n",
    "            print()\n",
    "        if iter_counter%2000 == 0:\n",
    "            em,f1 = evaluate(model,val_dataset)\n",
    "            model.train()\n",
    "            if f1>best_f1:\n",
    "                best_f1 = f1\n",
    "                model.save_pretrained('../custom/bert_qa_model_uncased')\n",
    "                print('best model!')\n",
    "            print()\n",
    "    print('epoch %d finish!'%(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model_bert = Bert_QA.from_pretrained('../custom/bert_qa_model_uncased')\n",
    "model_bert.to(device)\n",
    "model_bert.eval()\n",
    "evaluate(model_bert,val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasans_em = []\n",
    "hasans_f1 = []\n",
    "noans_em = []\n",
    "noans_f1 = []\n",
    "for i in tqdm(range(len(val_questions))):\n",
    "    pred,_ = online_predict(val_questions[i],val_contexts[i])\n",
    "    gold = val_answers[i]['text']\n",
    "    if gold == '':\n",
    "        noans_em.append(compute_exact(gold,pred))\n",
    "        noans_f1.append(compute_f1(gold,pred))\n",
    "    else:\n",
    "        hasans_em.append(compute_exact(gold,pred))\n",
    "        hasans_f1.append(compute_f1(gold,pred))\n",
    "\n",
    "print('has-ans em: %.2f'%(100*np.mean(hasans_em)))\n",
    "print('has-ans f1: %.2f'%(100*np.mean(hasans_f1)))\n",
    "print('no-ans em: %.2f'%(100*np.mean(noans_em)))\n",
    "print('no-ans f1: %.2f'%(100*np.mean(noans_f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    index = np.random.randint(10000)\n",
    "    c = val_contexts[index]\n",
    "    q = val_questions[index]\n",
    "    p_bert,_ = online_predict(q,c)\n",
    "    print('context:')\n",
    "    print(c)\n",
    "    print('question:')\n",
    "    print(q)\n",
    "    print('gold answer: %s'%val_answers[index]['text'])\n",
    "    print('bert answer: %s'%p_bert)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
